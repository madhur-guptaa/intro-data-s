{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ea9a1d3",
   "metadata": {
    "deletable": false
   },
   "source": [
    "# [Introduction to Data Science](http://datascience-intro.github.io/1MS041-2024/)    \n",
    "## 1MS041, 2024 \n",
    "&copy;2024 Raazesh Sainudiin, Benny Avelin. [Attribution 4.0 International     (CC BY 4.0)](https://creativecommons.org/licenses/by/4.0/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7292b131",
   "metadata": {},
   "source": [
    "## Concentration and limits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e54b52",
   "metadata": {},
   "source": [
    "Let us begin our story with the Long Term Relative Frequency idea, and consider an i.i.d. sequence of random variables $X_1,\\ldots,X_n$ each being Bernoulli($1/2$)."
   ]
  },
  {
   "cell_type": "code",
   "id": "b9ffe9a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T13:07:47.755200Z",
     "start_time": "2025-01-15T13:07:47.351172Z"
    }
   },
   "source": [
    "from ipywidgets import interact, IntSlider\n",
    "from Utils import discrete_histogram\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "@interact\n",
    "def concentration(n=IntSlider(1, 1, 500, 10, description=\"Samples\"), m=IntSlider(1, 1, 10, 1, description=\"Trials\")):\n",
    "    import matplotlib.pyplot as plt\n",
    "    np.random.seed(0)\n",
    "    # The following code is written in this cumbersome way to make sure that the \n",
    "    # random number generator is seeded in a way that is compatible with the\n",
    "    # idea that the previous trials remain the same when adding the next trial.\n",
    "    # This is important for the visualization.\n",
    "    X = np.concatenate([np.random.randint(0, 2, size=(n, 1)) for i in range(m)], axis=1)\n",
    "    means = np.cumsum(X, axis=0) / np.arange(1, n + 1)[:, None]\n",
    "    plt.plot(np.arange(1, n + 1), means, alpha=1 / np.sqrt(m))\n",
    "    plt.ylim(0, 1)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='Samples', max=500, min=1, step=10), IntSlider(value=1, d…"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "60bf93c58ec74eb8bf7bb4f8d4a1f790"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "b3e7c3dc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T13:07:47.901663Z",
     "start_time": "2025-01-15T13:07:47.856164Z"
    }
   },
   "source": [
    "from ipywidgets import interact, IntSlider\n",
    "from Utils import discrete_histogram\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "@interact\n",
    "def concentration(n=IntSlider(1, 1, 5000, 100, description=\"Samples\"),\n",
    "                  m=IntSlider(1, 1, 1000, 100, description=\"Trials\")):\n",
    "    import matplotlib.pyplot as plt\n",
    "    np.random.seed(0)\n",
    "    # The following is not stable w.r.t. trials, but works for larger m\n",
    "    X = np.random.randint(0, 2, size=(n, m))\n",
    "    means = np.cumsum(X, axis=0) / np.arange(1, n + 1)[:, None]\n",
    "    plt.plot(np.arange(1, n + 1), means, alpha=1 / np.sqrt(m))\n",
    "    plt.ylim(0, 1)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='Samples', max=5000, min=1, step=100), IntSlider(value=1,…"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e1b70bd41ea0437f846521ec0c4089a4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "f34070cb",
   "metadata": {},
   "source": [
    "Consider an i.i.d. sequence of random variables $X_1,\\ldots,X_n$ each being Bernoulli($1/2$). Then the concept of concentration is telling us that\n",
    "\n",
    "$$\n",
    "    P\\left ( \\left | \\frac{1}{n} \\sum_{i=1}^n X_i - \\mathbb{E}(X_i) \\right | > \\epsilon \\right )\n",
    "$$\n",
    "\n",
    "gets smaller as $n$ gets larger. For instance, using Chebychevs inequality we get\n",
    "\n",
    "$$\n",
    "    P\\left ( \\left | \\frac{1}{n} \\sum_{i=1}^n X_i - \\mathbb{E}(X_i) \\right | > \\epsilon \\right ) \\leq \\frac{\\mathbb{V}\\left( \\frac{1}{n} \\sum_{i=1}^n X_i \\right )}{\\epsilon^2} = \\frac{\\mathbb{V}\\left( X_0 \\right )}{\\epsilon^2 n}\n",
    "$$\n",
    "\n",
    "We can see that this is at least true in the simulation below"
   ]
  },
  {
   "cell_type": "code",
   "id": "9e489a8b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T13:07:47.998713Z",
     "start_time": "2025-01-15T13:07:47.916023Z"
    }
   },
   "source": [
    "from ipywidgets import interact, IntSlider\n",
    "from Utils import discrete_histogram\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "@interact\n",
    "def concentration(n=IntSlider(1, 1, 50, 5)):\n",
    "    import matplotlib.pyplot as plt\n",
    "    X = np.random.randint(0, 2, size=(n, 10000)) * 2 - 1\n",
    "    means = np.mean(X, axis=0)\n",
    "    print(\"P(mean > mu + 0.3 ) = %.5f <= Chebychev %.5f\" % (np.mean(means > 0.3), 1 / (0.3 ** 2 * n)))\n",
    "    discrete_histogram(means, normed=True)\n",
    "    plt.xlim(-1, 1)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n', max=50, min=1, step=5), Output()), _dom_classes=('wi…"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ca25923536b24b79ac3e361a27b2db35"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "5b914446",
   "metadata": {},
   "source": [
    "Hoeffdings inequality on the other hand is sharper and gives a bound much smaller, namely that\n",
    "\n",
    "$$\n",
    "    P\\left ( \\left |\\frac{1}{n} \\sum_{i=1}^n X_i - \\mathbb{E}(X_i) \\right | > \\epsilon \\right ) \n",
    "    \\leq 2e^{-2n \\epsilon^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "id": "129b66d8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T13:07:48.089222Z",
     "start_time": "2025-01-15T13:07:48.011022Z"
    }
   },
   "source": [
    "@interact\n",
    "def concentration(n=IntSlider(1, 1, 50, 5)):\n",
    "    import matplotlib.pyplot as plt\n",
    "    X = np.random.randint(0, 2, size=(n, 10000))\n",
    "    means = np.mean(X, axis=0)\n",
    "    print(\"P(mean > mu + 0.3 ) = %.5f <= Hoeffding %.5f\" % (np.mean(means > 0.5 + 0.3), np.exp(-2 * n * 0.3 ** 2)))\n",
    "    discrete_histogram(means, normed=True)\n",
    "    plt.xlim(0, 1)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n', max=50, min=1, step=5), Output()), _dom_classes=('wi…"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "585263c3b8744baa9fe847ed6ebaee99"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "id": "d8b35afc",
   "metadata": {},
   "source": [
    "This is much closer, and in fact very close."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0984d87e",
   "metadata": {},
   "source": [
    "### Using concentration as a measure of confidence\n",
    "\n",
    "We can use concentration as a measure of confidence in the following way. Consider $X_1,\\ldots, X_n$ being i.i.d. sequence of Bernoulli($p$) for some unknown $p$. From the concept of concentration, we would expect that if we have many observations ($n$ large) we could use the empirical mean of the observations as a guess, but note that there is some variability as we saw in the above simulations. So what do we do? We use the concentration inequality to get information how far we can deviate from $p$ in the following way\n",
    "\n",
    "$$\n",
    "    P(\\bar X_n - \\mathbb{E}(\\bar X_n) \\geq \\epsilon) \\leq e^{-2n\\epsilon^2}\n",
    "$$\n",
    "\n",
    "Since $\\mathbb{E}(\\bar X_n) = p$, rearrange and get\n",
    "\n",
    "$$\n",
    "    P(p \\leq \\bar X_n - \\epsilon) \\leq e^{-2n\\epsilon^2}\n",
    "$$\n",
    "\n",
    "The complementary event thus satisfies\n",
    "\n",
    "$$\n",
    "    P(\\bar X_n - \\epsilon < p) \\geq 1-e^{-2n\\epsilon^2}\n",
    "$$\n",
    "\n",
    "We can do the same for the other side (see lecture notes) and we get\n",
    "\n",
    "$$\n",
    "    P(\\bar X_n - \\epsilon < p < \\bar X_n + \\epsilon) \\geq 1-2 e^{-2n\\epsilon^2}.\n",
    "$$\n",
    "\n",
    "If you where now asked to estimate $p$ using $n$ observations and give an interval where you with at least 95% confidence can say contains $p$, then you need to choose $\\epsilon > 0$ such that\n",
    "\n",
    "$$\n",
    "    1-2 e^{-2n\\epsilon^2} \\geq 0.95.\n",
    "$$\n",
    "\n",
    "Smaller $\\epsilon$ gives smaller intervals, so lets choose to have the smallest possible $\\epsilon$ while still obaying the inequality above, i.e. we choose $\\epsilon$ to solve\n",
    "\n",
    "$$\n",
    "    1-2 e^{-2n\\epsilon^2} = 0.95.\n",
    "$$\n",
    "\n",
    "Rearranging we and taking log and then square root we obtain\n",
    "\n",
    "$$\n",
    "    \\epsilon = \\sqrt{-\\frac{1}{2n}\\ln\\left(\\frac{1-0.95}{2}\\right)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "id": "0574fa7d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T13:07:48.107111Z",
     "start_time": "2025-01-15T13:07:48.100777Z"
    }
   },
   "source": [
    "from ipywidgets import FloatSlider\n",
    "\n",
    "\n",
    "@interact\n",
    "def concentration(n=IntSlider(value=1, min=50, max=500, step=50), p=FloatSlider(value=0.5, min=0, max=1, step=0.1)):\n",
    "    X = np.random.binomial(1, p, size=(n))\n",
    "    means = np.mean(X, axis=0)\n",
    "    epsilon = np.sqrt(-1 / (2 * n) * np.log((1 - 0.95) / 2))\n",
    "    print(\"95%% confidence interval [%.2f, %.2f] for p=%.2f n=%d\" % (means - epsilon, means + epsilon, p, n))"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "interactive(children=(IntSlider(value=50, description='n', max=500, min=50, step=50), FloatSlider(value=0.5, d…"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d5ecb90a4ca84827bed4de3b33283795"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "03574a64",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T13:07:48.125236Z",
     "start_time": "2025-01-15T13:07:48.118657Z"
    }
   },
   "source": [
    "from ipywidgets import FloatSlider\n",
    "\n",
    "\n",
    "@interact\n",
    "def concentration_300(n=IntSlider(value=1, min=50, max=5000, step=50)):\n",
    "    X = np.minimum(np.abs(70 + 6 * np.random.normal(size=(n))), 300)\n",
    "    means = np.mean(X, axis=0)\n",
    "    epsilon = np.sqrt(-1 / (2 * n / (300 ** 2)) * np.log((1 - 0.95) / 2))\n",
    "    print(\"95%% confidence interval [%.2f, %.2f] for n=%d\" % (means - epsilon, means + epsilon, n))"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "interactive(children=(IntSlider(value=50, description='n', max=5000, min=50, step=50), Output()), _dom_classes…"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f3cbc4a3b9b947c7bd6b1a7a8da2b9e7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "id": "6960fcfa",
   "metadata": {},
   "source": [
    "These things are super useful. Lets go back to our sms spam/ham problem and see what we can say there"
   ]
  },
  {
   "cell_type": "code",
   "id": "d6785c64",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T13:07:48.149225Z",
     "start_time": "2025-01-15T13:07:48.140211Z"
    }
   },
   "source": [
    "from Utils import load_sms\n",
    "\n",
    "sms_data = load_sms()\n",
    "sms_data[:2]"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...',\n",
       "  0),\n",
       " ('Ok lar... Joking wif u oni...', 0)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "8610dd2e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T13:07:48.168471Z",
     "start_time": "2025-01-15T13:07:48.156669Z"
    }
   },
   "source": [
    "interesting_words = set(['free', 'prize'])\n",
    "TF10 = {True: 1, False: 0}\n",
    "Z_obs = [TF10[not interesting_words.isdisjoint([word.lower() for word in line[0].split(' ')])] for line in sms_data]\n",
    "Y_obs = [y for x, y in sms_data]"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "id": "07f78671",
   "metadata": {},
   "source": [
    "Recall that we computed the mean of Y_obs, which is the mean spam number. This is a Bernoulli random variable with unknown $p$, so we can use our methods above to compute a confidence interval"
   ]
  },
  {
   "cell_type": "code",
   "id": "5048f6e8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T13:07:48.174889Z",
     "start_time": "2025-01-15T13:07:48.173272Z"
    }
   },
   "source": [
    "def epsilon_bernoulli(n, alpha):\n",
    "    return np.sqrt(-1 / (2 * n) * np.log((alpha) / 2))"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "id": "b41b4802",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T13:07:48.181373Z",
     "start_time": "2025-01-15T13:07:48.179297Z"
    }
   },
   "source": [
    "epsilon = epsilon_bernoulli(len(Y_obs), 0.05)\n",
    "mean_Y_obs = np.mean(Y_obs)\n",
    "print(\"[%.3f,%.3f]\" % (mean_Y_obs - epsilon, mean_Y_obs + epsilon))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.116,0.152]\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "id": "1d22f090",
   "metadata": {},
   "source": [
    "From this we get that we have provided a prediction as to what is the true probability of getting a spam email.\n",
    "\n",
    "Lets take a look at the conditional probability. Recall that\n",
    "\n",
    "$$\n",
    "    P(Y = 1 \\mid Z = 1) = \\frac{P(Y = 1 \\text{ and } Z = 1)}{P(Z = 1)}\n",
    "$$\n",
    "\n",
    "but this requires you to estimate both things on the right, and give a region for both and finally figure out an interval for the ratio. But, there is an easier way, which sometimes works better. Namely, to look at the random variable $Y \\mid (Z=1)$ which we do by filtering"
   ]
  },
  {
   "cell_type": "code",
   "id": "f494a43b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T13:07:48.204030Z",
     "start_time": "2025-01-15T13:07:48.201866Z"
    }
   },
   "source": "Y_mid_Z1 = [y for z, y in zip(Z_obs, Y_obs) if z == 1]",
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "id": "c75bba93",
   "metadata": {},
   "source": [
    "Now for this you have a certain number of observations and we can use this instead as follows"
   ]
  },
  {
   "cell_type": "code",
   "id": "0ed5efd2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T13:07:48.222214Z",
     "start_time": "2025-01-15T13:07:48.219907Z"
    }
   },
   "source": [
    "epsilon = epsilon_bernoulli(len(Y_mid_Z1), 0.05)\n",
    "mean_Y_obs = np.mean(Y_mid_Z1)\n",
    "print(\"[%.3f,%.3f]\" % (mean_Y_obs - epsilon, mean_Y_obs + epsilon))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.726,0.898]\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "id": "d8b9b727",
   "metadata": {},
   "source": [
    "## Being clever with Bennetts inequality"
   ]
  },
  {
   "cell_type": "code",
   "id": "fd593d5c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T13:07:48.241637Z",
     "start_time": "2025-01-15T13:07:48.239537Z"
    }
   },
   "source": [
    "def bennett_epsilon(n, b, sigma, alpha):\n",
    "    import scipy.optimize as so\n",
    "    h = lambda u: (1 + u) * np.log(1 + u) - u\n",
    "    f = lambda epsilon: np.exp(-n * sigma ** 2 / b ** 2 * h(b * epsilon / sigma ** 2)) - alpha / 2\n",
    "    ans = so.fsolve(f, 0.002)\n",
    "    epsilon = np.abs(ans[0])\n",
    "    print(\"Numerical error\", f(epsilon))\n",
    "    return epsilon"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "id": "162aadcb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T13:07:48.480847Z",
     "start_time": "2025-01-15T13:07:48.254578Z"
    }
   },
   "source": "bennett_epsilon(50, 300, 20, 0.05)",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numerical error -4.5102810375396984e-17\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(13.457174445041813)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "id": "085c43bb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T13:07:48.494496Z",
     "start_time": "2025-01-15T13:07:48.486789Z"
    }
   },
   "source": [
    "from ipywidgets import FloatSlider\n",
    "\n",
    "\n",
    "@interact\n",
    "def concentration_300(n=IntSlider(value=1, min=50, max=5000, step=50),\n",
    "                      sigma=FloatSlider(value=10, min=1, max=300, step=1)):\n",
    "    X = np.minimum(np.abs(70 + 6 * np.random.normal(size=(n))), 300)\n",
    "    means = np.mean(X, axis=0)\n",
    "    epsilon = np.sqrt(-1 / (2 * n / (300 ** 2)) * np.log((1 - 0.95) / 2))\n",
    "    epsilon2 = bennett_epsilon(n, b=300, sigma=sigma, alpha=0.05)\n",
    "    print(\"95%% confidence interval rough [%.2f, %.2f] for n=%d\" % (means - epsilon, means + epsilon, n))\n",
    "    print(\"95%% confidence interval Bennett [%.2f, %.2f] for n=%d\" % (means - epsilon2, means + epsilon2, n))"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "interactive(children=(IntSlider(value=50, description='n', max=5000, min=50, step=50), FloatSlider(value=10.0,…"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a796572209614787a2e8330c35a11719"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 15
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "lx_course_instance": "2024",
  "lx_course_name": "Introduction to Data Science",
  "lx_course_number": "1MS041"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
